{"cells":[{"cell_type":"markdown","metadata":{"id":"dlBtL1DG_7FR"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"dPYUasWIvdRJ"},"outputs":[],"source":["! pip install --upgrade -q datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTDFFnW2rKmf","tags":[]},"outputs":[],"source":["import pandas as pd\n","import re\n","import json\n","import os\n","import glob\n","from tqdm import tqdm\n","from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["def recalculate_line_positions_with_newlines(data_lines):\n","    \"\"\"\n","    Recalculate the start positions of each line in a list of text lines, considering the newline characters.\n","\n","    Parameters:\n","    data_lines (list of str): A list of strings, where each string represents a line from a text file.\n","\n","    Returns:\n","    list: A list of integers, each representing the starting character position of a line in the combined text.\n","          The last position (which would be the end of the file) is not included.\n","\n","    Example:\n","    If data_lines = [\"Hello\", \"World\"], the function will return [0, 6].\n","    \"Hello\" starts at position 0, and \"World\" starts at position 6 (5 characters of \"Hello\" + 1 newline character).\n","    \"\"\"\n","\n","    positions = [0] # Initialize with 0 for the start of the first line\n","    current_position = 0\n","\n","    for line in data_lines:\n","        current_position += len(line) + 1  # Increment by line length plus newline character\n","        positions.append(current_position)\n","\n","    return positions[:-1]  # Exclude the last position which is end of file, not start of a new line\n","\n","def parse_txt(data_files, answer_file=None):\n","    \"\"\"\n","    Parse text files to extract specific annotations and prepare data for further processing.\n","\n","    This function reads text files and associated answer files (if provided) to create a structured dataset.\n","\n","    Parameters:\n","    data_files (list of str): List of paths to text files that need to be processed.\n","    answer_file (str, optional): Path to a file containing answers or annotations for the text files.\n","                                 The structure of this file should be tab-separated values\n","                                 with at least 5 columns: file_id, tag_type, start_line, end_line, and text_info.\n","                                 An optional sixth column for normalized_time can also be included.\n","\n","    Returns:\n","    list: A list of dictionaries, each containing a 'prompt' key with text to process and a 'completion' key\n","          with the corresponding extracted information.\n","\n","    Each text file is processed to find lines that contain information specified in the answer_file.\n","    If an answer is found in a line, the corresponding metadata is extracted and added to the output.\n","    If the answer file is provided, the function searches for specific tagged information in each line\n","    and compiles found tags along with their positions.\n","    \"\"\"\n","\n","    output_data = []\n","    answers = {}\n","\n","    if answer_file:\n","        # Load contents from the answer file\n","        with open(answer_file, 'r', encoding='utf-8') as file:\n","            for line in file:\n","                parts = line.strip().split('\\t')\n","                if len(parts) < 5:\n","                    continue\n","                file_id, tag_type, start_line, end_line, text_info = parts[:5]\n","                normalized_time = parts[5] if len(parts) == 6 else None\n","                if file_id not in answers:\n","                    answers[file_id] = []\n","                answers[file_id].append({\n","                    'tag_type': tag_type,\n","                    'start_line': int(start_line),\n","                    'end_line': int(end_line),\n","                    'text_info': text_info,\n","                    'normalized_time': normalized_time\n","                })\n","\n","    for data_file in tqdm(data_files, desc=\"Processing files\"):\n","        # Process each .txt file\n","        file_name_no_ext = os.path.basename(data_file).split('.')[0]\n","        associated_answers = answers.get(file_name_no_ext, [])\n","\n","        with open(data_file, 'r', encoding='utf-8') as file:\n","            data_content = file.read()\n","            data_lines = data_content.split('\\n')\n","            line_positions = recalculate_line_positions_with_newlines(data_lines)\n","\n","        for line_number, line_content in enumerate(data_lines):\n","            line_start_pos = line_positions[line_number]\n","            if not line_content.strip():\n","                continue\n","\n","            found_answers = []\n","            for ans in associated_answers:\n","                if ans['text_info'] in line_content:\n","                    if not any(a['tag_type'] == ans['tag_type'] and a['text_info'] == ans['text_info'] for a in found_answers):\n","                        start_index = line_content.find(ans['text_info'])\n","                        if start_index != -1:\n","                            ans['start_line'] = line_start_pos + start_index\n","                            ans['end_line'] = ans['start_line'] + len(ans['text_info'])\n","                            found_answers.append(ans)\n","\n","            json_entry = {\n","                \"prompt\": f\"Please extract HIPAA related information from the given text: file ID:{file_name_no_ext}, start:{line_start_pos}, content:{line_content}\\n\\n###\\n\\n\"\n","            }\n","\n","            if found_answers:\n","                ans_text = ' | '.join([f\"Type: {ans['tag_type']}, Content: {ans['text_info']}\" + (f\" (Normalized: {ans['normalized_time']})\" if ans['normalized_time'] else \"\") for ans in found_answers])\n","                json_entry[\"completion\"] = f\"{ans_text} END\"\n","            elif answer_file:\n","                json_entry[\"completion\"] = \"PHI: NULL\"\n","\n","            output_data.append(json_entry)\n","\n","    return output_data\n"],"metadata":{"id":"bpEuYZGqzaY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define file paths and load datasets\n","file_path = \"/mnt/nas/HYZ/AICUP/Dataset/\"\n","\n","train_data_files1 = glob.glob(f'{file_path}Train/First_Dataset/*.txt')\n","train_answer_file1 = f'{file_path}Train/First_answer.txt'\n","train_data1 = parse_txt(train_data_files1, train_answer_file1)\n","\n","train_data_files2 = glob.glob(f'{file_path}Train/Second_Dataset/*.txt')\n","train_answer_file2 = f'{file_path}Train/Second_answer.txt'\n","train_data2 = parse_txt(train_data_files2, train_answer_file2)\n","\n","val_data_files = glob.glob(f'{file_path}Validation/*.txt')\n","val_answer_file = f'{file_path}Validation_answer.txt'\n","val_data = parse_txt(val_data_files, val_answer_file)\n","\n","test_data_files = glob.glob(f'{file_path}Test/*.txt')\n","test_data = parse_txt(test_data_files, answer_file=None)"],"metadata":{"id":"ScUa_hvO0SNr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4joeIgCtvdRN"},"outputs":[],"source":["# Combine and convert datasets to pandas DataFrame\n","train_data_combined = train_data1 + train_data2 + val_data\n","train_df = pd.DataFrame(train_data_combined)\n","test_df = pd.DataFrame(test_data)\n","\n","# Convert DataFrames to Hugging Face Dataset format\n","train_dataset = Dataset.from_pandas(train_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","# Split training data into train and validation sets\n","train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1)\n","\n","# Convert the split datasets back to Hugging Face Dataset format\n","train_dataset = Dataset.from_dict(train_dataset)\n","val_dataset = Dataset.from_dict(val_dataset)\n","\n","# Create a DatasetDict for easy handling of the datasets\n","dataset_dict = DatasetDict({\n","    \"train\": train_dataset,\n","    \"validation\": val_dataset,\n","    \"test\": test_dataset\n","})\n","\n","# Save the dataset dictionary to disk\n","dataset_dir = f\"/mnt/nas/HYZ/AICUP/dataset_dict_v2\"\n","dataset_dict.save_to_disk(dataset_dir)"]},{"cell_type":"code","source":["# Loading the saved dataset dictionary from disk\n","dataset_dir = \"/mnt/nas/HYZ/AICUP/dataset_dict_v2\"\n","dataset = load_from_disk(dataset_dir)\n","\n","dataset  # Display the loaded dataset"],"metadata":{"id":"SL16D51Z1BHH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}