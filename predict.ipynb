{"cells":[{"cell_type":"code","execution_count":null,"id":"bcc420a4-f0b0-4dfd-a330-8aacb1e44ca7","metadata":{"tags":[],"id":"bcc420a4-f0b0-4dfd-a330-8aacb1e44ca7"},"outputs":[],"source":["!pip install -q transformers datasets rouge_score evaluate accelerate scikit-learn pycountry"]},{"cell_type":"code","execution_count":null,"id":"6be12cac-09c0-4189-a413-b80c88ccd7a6","metadata":{"tags":[],"id":"6be12cac-09c0-4189-a413-b80c88ccd7a6"},"outputs":[],"source":["# Use Fine Tune on Train + Validarion model\n","fine_tune_model_dir = \"/mnt/nas/HYZ/AICUP/save_finetune_model/\"\n","\n","# # base on 1701326984 hyperparameter Exact Match 94.286300 16/2e-5/0.01/15\n","# fine_tune_model_name = \"1701406936-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","\n","# # base on 1700994394 hyperparameter, Exact Match 94.431500 4/2e-5/0.01/10\n","# fine_tune_model_name = \"1701407311-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","\n","# # base on 1700994394 hyperparameter, Exact Match 94.567100, 4/2e-5/0.05/10\n","fine_tune_model_name = \"1701489459-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","config = {\n","    \"model_checkpoint\": f\"{fine_tune_model_dir}{fine_tune_model_name}\",\n","    \"max_input_length\": 512,\n","    \"max_target_length\": 100,\n","    \"batch_size\": 16,\n","    \"generation_max_length\": 100\n","}"]},{"cell_type":"code","execution_count":null,"id":"e2c17052-6020-48ec-9d6e-d99887e3e5f3","metadata":{"tags":[],"id":"e2c17052-6020-48ec-9d6e-d99887e3e5f3","outputId":"86b8dcb3-c53f-4874-d614-7011b67b3e8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n","\n","print(torch.cuda.device_count())"]},{"cell_type":"code","execution_count":null,"id":"98774647-4036-44e0-8ebf-bc586c0e0db4","metadata":{"tags":[],"id":"98774647-4036-44e0-8ebf-bc586c0e0db4","outputId":"31b209ac-d0ac-4cd8-8672-986e28267244"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2023-12-03 04:07:02,524] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"]}],"source":["from transformers import (\n","    AutoTokenizer, AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, GenerationConfig\n",")\n","from datasets import load_dataset, DatasetDict, Dataset, load_from_disk, concatenate_datasets\n","from tqdm import tqdm\n","from datetime import datetime\n","import pandas as pd\n","import numpy as np\n","import pycountry\n","import re\n","import json\n","import zipfile"]},{"cell_type":"code","execution_count":null,"id":"32859ae8-9215-4ff3-b0fa-df0cd9f71349","metadata":{"tags":[],"id":"32859ae8-9215-4ff3-b0fa-df0cd9f71349","outputId":"14b96dda-dbb4-4752-d118-888429ad103a"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['prompt', 'completion'],\n","        num_rows: 92933\n","    })\n","    validation: Dataset({\n","        features: ['prompt', 'completion'],\n","        num_rows: 10326\n","    })\n","    test: Dataset({\n","        features: ['prompt'],\n","        num_rows: 47084\n","    })\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Loading dataset\n","file_path = '/mnt/nas/HYZ/AICUP/'\n","dataset = load_from_disk(f\"{file_path}dataset_dict_v2\")\n","\n","dataset"]},{"cell_type":"code","execution_count":null,"id":"ecbea832-8c01-42ff-90d8-f5c17db11bf5","metadata":{"tags":[],"id":"ecbea832-8c01-42ff-90d8-f5c17db11bf5"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(config[\"model_checkpoint\"])\n","model = AutoModelForSeq2SeqLM.from_pretrained(config[\"model_checkpoint\"])"]},{"cell_type":"code","execution_count":null,"id":"e3a226be-046b-43d8-afdf-578e8871828e","metadata":{"tags":[],"id":"e3a226be-046b-43d8-afdf-578e8871828e","outputId":"63a5e3cd-b173-4226-b7b7-1c40bb8b6b31","colab":{"referenced_widgets":["14c13e13ef014eb99d3f4bba18754e40","02e6fbfedf4f409193fbc263a64bd977","78f27d6e0cdd4b2ab27c3397cfd1c0a0"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14c13e13ef014eb99d3f4bba18754e40","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/92933 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02e6fbfedf4f409193fbc263a64bd977","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10326 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78f27d6e0cdd4b2ab27c3397cfd1c0a0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/47084 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Keys of tokenized dataset: ['prompt', 'input_ids', 'attention_mask']\n"]}],"source":["def preprocess_function(examples):\n","    \"\"\"\n","    Preprocesses the input data for training or evaluating the T5 model.\n","\n","    This function tokenizes the inputs and labels (if available) using the specified tokenizer.\n","    It's designed to be used with datasets in the Hugging Face 'datasets' library,\n","    where each item is a dictionary with 'prompt' and optionally 'completion' keys.\n","\n","    Parameters:\n","    examples (dict): A dictionary containing 'prompt' and optionally 'completion' keys.\n","                     The values are lists of strings: the inputs and the expected outputs for the model.\n","\n","    Returns:\n","    dict: A dictionary with tokenized 'input_ids' and optionally 'labels' for training/evaluation.\n","\n","    The function tokenizes 'prompt' to create the model inputs.\n","    If 'completion' is present, it's also tokenized to create the labels for training.\n","    For labels, padding tokens are replaced with -100 to ignore them in the loss computation.\n","    \"\"\"\n","\n","    # Tokenize the input text\n","    model_inputs = tokenizer(examples[\"prompt\"], padding=\"max_length\", max_length=config[\"max_input_length\"], truncation=True)\n","\n","    if \"completion\" in examples:\n","        # Tokenize the labels (if present)\n","        labels = tokenizer(examples[\"completion\"], padding=\"max_length\", max_length=config[\"max_target_length\"], truncation=True)\n","\n","        # Replace padding token id with -100 in labels\n","        labels[\"input_ids\"] = [\n","            [(label if label != tokenizer.pad_token_id else -100) for label in label_example] for label_example in labels[\"input_ids\"]\n","        ]\n","\n","        # Add labels to model inputs\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs\n","\n","# Applying the preprocessing function to the datasets\n","tokenized_dataset = dataset.map(preprocess_function, batched=True)\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"]},{"cell_type":"code","execution_count":null,"id":"7c2f4d5f-109b-4e60-aa73-fe95b7be2580","metadata":{"tags":[],"id":"7c2f4d5f-109b-4e60-aa73-fe95b7be2580","outputId":"78ce0909-2cdf-49ff-db22-13741bc22837"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model List: ['1701489459', 't5', 'efficient', 'base', 'dl2', 'finetuned', 'extracted', 'PHI']\n","Model Name: 1701489459\n"]}],"source":["# Setting up training arguments for fine-tuning\n","model_list = config[\"model_checkpoint\"].split(\"/\")[-1].split(\"-\")\n","model_name = model_list[0]\n","print(f\"Model List: {model_list}\")\n","print(f\"Model Name: {model_name}\")"]},{"cell_type":"code","execution_count":null,"id":"16e009fa-dc85-47c0-917f-f9b9bb3fa20e","metadata":{"tags":[],"id":"16e009fa-dc85-47c0-917f-f9b9bb3fa20e","outputId":"b7f96d8e-3826-4362-f015-e11f0bd9339f"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=Seq2SeqTrainingArguments(\n","        # output_dir=\"finetune-model-predicted\",\n","        output_dir=\"Result\",\n","        per_device_eval_batch_size=config[\"batch_size\"]*config[\"batch_size\"],\n","        predict_with_generate=True,\n","        generation_max_length=config[\"generation_max_length\"],\n","    ),\n","    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, pad_to_multiple_of=8),\n","    tokenizer=tokenizer,\n",")\n","\n","test_predictions = trainer.predict(tokenized_dataset[\"test\"])"]},{"cell_type":"code","execution_count":null,"id":"f196e0d8-e787-4f6d-a347-119ce2fbd372","metadata":{"tags":[],"id":"f196e0d8-e787-4f6d-a347-119ce2fbd372","outputId":"11bd8b27-a82c-4880-8807-d070a9a37a0b"},"outputs":[{"data":{"text/plain":["array([    0,  6632,    10,    27, 12145,  6122,     6,  7185,    10,\n","         335,   434,  2445,  3959,  2884,   329,  1820,  6632,    10,\n","          27, 12145,  6122,     6,  7185,    10,   335,   434,  2445,\n","        3959,   357,     3, 14920,     1,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["test_predictions[0][0]"]},{"cell_type":"code","execution_count":null,"id":"1b5f749a-0fc1-47af-9c27-207c29c8ae9a","metadata":{"tags":[],"id":"1b5f749a-0fc1-47af-9c27-207c29c8ae9a","outputId":"a6b9df7b-01de-4c4c-cb83-17f04aa77ae9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Type: IDNUM, Content: 10L407622M | Type: IDNUM, Content: 10L40762 END\n"]}],"source":["# Decoding and filtering the test predictions\n","decoded_predictions = []\n","for pred in test_predictions.predictions:\n","    filtered_pred = [token_id for token_id in pred if token_id != -100 and 0 <= token_id < tokenizer.vocab_size]\n","    decoded_text = tokenizer.decode(filtered_pred, skip_special_tokens=True)\n","    decoded_predictions.append(decoded_text)\n","\n","print(decoded_predictions[0])"]},{"cell_type":"markdown","id":"2e570617-7d2f-4866-8967-080d3ac752be","metadata":{"tags":[],"id":"2e570617-7d2f-4866-8967-080d3ac752be"},"source":["## Save Result"]},{"cell_type":"code","execution_count":null,"id":"5128c098-dad0-4e3c-85f0-4c69266fb40e","metadata":{"tags":[],"id":"5128c098-dad0-4e3c-85f0-4c69266fb40e"},"outputs":[],"source":["# Creating a DataFrame with prompts and their decoded completions\n","test_df = pd.DataFrame({\n","    \"prompt\": tokenized_dataset[\"test\"][\"prompt\"],\n","    \"completion\": decoded_predictions\n","})\n","\n","# test_df"]},{"cell_type":"code","execution_count":null,"id":"3ab426b0-39d3-45b8-b6a7-dba81c96c6fb","metadata":{"id":"3ab426b0-39d3-45b8-b6a7-dba81c96c6fb"},"outputs":[],"source":["# Save jsonl\n","output_file = f\"./Result/{model_name}-{config['generation_max_length']}-original_predicted.jsonl\"\n","test_df.to_json(output_file, orient=\"records\", lines=True)"]},{"cell_type":"markdown","id":"575e25d3-89e4-47bf-9d0f-742d29554524","metadata":{"id":"575e25d3-89e4-47bf-9d0f-742d29554524"},"source":["# Rule-base"]},{"cell_type":"code","execution_count":null,"id":"8b19c92c-eb1a-4263-a0ed-68b4d6985ee5","metadata":{"id":"8b19c92c-eb1a-4263-a0ed-68b4d6985ee5","outputId":"bea61c02-c75b-4814-adb2-c305ac1da083"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 47084/47084 [00:02<00:00, 21619.40it/s]\n","100%|██████████| 47084/47084 [00:03<00:00, 12598.18it/s]\n","100%|██████████| 47084/47084 [00:02<00:00, 20216.91it/s]\n","100%|██████████| 47084/47084 [00:02<00:00, 18242.52it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>completion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>Type: IDNUM, Content: 10L407622M | Type: IDNUM...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>Type: MEDICALRECORD, Content: 10195149 END</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>Type: HOSPITAL, Content: COLAC AREA HEALTH END</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>47079</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47080</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47081</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47082</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47083</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>47084 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  prompt  \\\n","0      Please extract HIPAA related information from ...   \n","1      Please extract HIPAA related information from ...   \n","2      Please extract HIPAA related information from ...   \n","3      Please extract HIPAA related information from ...   \n","4      Please extract HIPAA related information from ...   \n","...                                                  ...   \n","47079  Please extract HIPAA related information from ...   \n","47080  Please extract HIPAA related information from ...   \n","47081  Please extract HIPAA related information from ...   \n","47082  Please extract HIPAA related information from ...   \n","47083  Please extract HIPAA related information from ...   \n","\n","                                              completion  \n","0      Type: IDNUM, Content: 10L407622M | Type: IDNUM...  \n","1             Type: MEDICALRECORD, Content: 10195149 END  \n","2         Type: HOSPITAL, Content: COLAC AREA HEALTH END  \n","3                                              PHI: NULL  \n","4                                              PHI: NULL  \n","...                                                  ...  \n","47079                                          PHI: NULL  \n","47080                                          PHI: NULL  \n","47081                                          PHI: NULL  \n","47082                                          PHI: NULL  \n","47083                                          PHI: NULL  \n","\n","[47084 rows x 2 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Sub-function: Extract Phone Number\n","def extracted_phone(row):\n","    \"\"\"\n","    Extracts phone numbers in the format 'dddd dddd' from the 'prompt' field of a given row.\n","    If a phone number is found, the row's 'completion' field is updated with the phone number details.\n","    \"\"\"\n","    phone_pattern = r'\\b\\d{4} \\d{4}\\b'\n","    phones = re.findall(phone_pattern, row['prompt'])\n","    if phones:\n","        row['completion'] = f\"Type: PHONE, Content: {phones[0]} END\"\n","    return row\n","\n","# Sub-function: Extract Country Name\n","def extracted_country(row, countries, exceptions):\n","    \"\"\"\n","    Extracts country names from the 'prompt' field of a given row, considering certain rules and exceptions.\n","    Skips cases where 'Dr', 'DR', or 'PRO' is followed by a country name and handles special word patterns.\n","    \"\"\"\n","    # Check if the string contains \"Dr\", \"DR\", or \"PRO\"\n","    if re.search(r'\\b(Dr|DR|PRO)\\b', row['prompt']):\n","        # Further checks if the word following \"Dr\", \"DR\", or \"PRO\" is a country name\n","        dr_or_pro_match = re.search(r'\\b(Dr|DR|PRO)\\s+(\\w+)', row['prompt'])\n","        if dr_or_pro_match and dr_or_pro_match.group(2) in countries:\n","            return row  # If a country name is matched, skip\n","\n","    # Check for country names following \"with\"\n","    if re.search(r'\\bwith\\s+(\\w+)', row['prompt']):\n","        with_match = re.search(r'\\bwith\\s+(\\w+)', row['prompt'])\n","        if with_match and with_match.group(1) in countries:\n","            if re.search(r'\\bwith\\s+\\w+\\s+\\(\\w+\\)', row['prompt']):\n","                return row\n","\n","    # Check for exceptions\n","    if any(exception in row['prompt'] for exception in exceptions):\n","        return row\n","\n","    # Check for country names in specific patterns\n","    if re.search(r'\\bwith\\s+(\\w+)(?!\\s+\\()', row['prompt']):\n","        with_match = re.search(r'\\bwith\\s+(\\w+)(?!\\s+\\()', row['prompt'])\n","        if with_match and with_match.group(1) in countries:\n","            return row\n","    return row\n","\n","# Sub-function: Extract Other Locations\n","def extracted_location_other(row):\n","    \"\"\"\n","    Extracts other location details like P.O. BOX numbers and specific words from the 'prompt' field of a row.\n","    Updates the 'completion' field with these details if found.\n","    \"\"\"\n","    po_box_pattern = r'\\bP\\.?\\s*O\\.?\\s*BOX\\s+\\d+\\b'\n","    janborwill_pattern = r'\\bJANBORWILL\\b'\n","    po_box_matches = re.findall(po_box_pattern, row['prompt'], re.IGNORECASE)\n","    janborwill_matches = re.findall(janborwill_pattern, row['prompt'], re.IGNORECASE)\n","\n","    contents = po_box_matches + janborwill_matches\n","    if contents:\n","        row['completion'] = f\"Type: LOCATION-OTHER, Content: {', '.join(contents)} END\"\n","\n","    return row\n","\n","# Sub-function: Extract and Normalize Duration\n","def extract_and_normalized_duration(row):\n","    \"\"\"\n","    Extracts and normalizes duration details from the 'prompt' field of a row.\n","    Converts textual duration descriptions to a standard format.\n","    \"\"\"\n","    word_to_number = {'one': '1', 'two': '2', 'three': '3', 'four': '4',\n","                      'twenty': '20', 'forty': '40'}\n","\n","    duration_pattern = r'\\b((\\d+(-\\d+)?|one|two|three|twenty|four)\\s+(months?|years?|yrs?|weeks?|wks?|days?)|(\\d+)(?<!\\s)(yr|year))(?!\\s*(old|\\s*-|\\s*cycle|\\s*cycles))\\b'\n","    duration_matches = re.findall(duration_pattern, row['prompt'], re.IGNORECASE)\n","\n","    normalized_durations = []\n","    for match in duration_matches:\n","        text_num = match[1] if match[1] else match[4]\n","        unit_text = match[3] if match[3] else 'year'\n","\n","        # Convert textual number descriptions to numeric format\n","        num = word_to_number.get(text_num.lower(), text_num)\n","        unit = unit_text[0].upper()\n","\n","        normalized = \"P\" + num + unit\n","        duration_format = f\"{text_num} {unit_text}\"\n","        normalized_durations.append(f\"{duration_format} (Normalized: {normalized})\")\n","\n","    if normalized_durations:\n","        row['completion'] = f\"Type: DURATION, Content: {', '.join(normalized_durations)} END\"\n","\n","    return row\n","\n","\n","# Integrated Function: Extract Information\n","def extract_info(df):\n","    \"\"\"\n","    Integrates various sub-functions to extract different types of information from a DataFrame.\n","    Handles country names, phone numbers, other locations, and durations.\n","    \"\"\"\n","    countries = [country.name for country in pycountry.countries]\n","    countries.sort(key=len, reverse=True)\n","    countries.append(\"Vietnam\") # pycountry is Viet Nam\n","    countries.append(\"USA\")\n","    exceptions = [\"Western Australia\", \"South Australia\", \"Congo red\", \"Congo Red\"]\n","\n","    tqdm.pandas()\n","\n","    for func in [extracted_phone, lambda row: extracted_country(row, countries, exceptions), extracted_location_other, extract_and_normalized_duration]:\n","        df = df.progress_apply(func, axis=1)\n","    return df\n","\n","# Applying the Function\n","test_df = extract_info(test_df)\n","test_df"]},{"cell_type":"code","execution_count":null,"id":"5e98745b-7a2e-4d68-9b6f-ae5de10087c6","metadata":{"tags":[],"id":"5e98745b-7a2e-4d68-9b6f-ae5de10087c6"},"outputs":[],"source":["# Save jsonl\n","output_file = f\"./Result/{model_name}-{config['generation_max_length']}-after_rule_predicted.jsonl\"\n","test_df.to_json(output_file, orient=\"records\", lines=True)"]},{"cell_type":"code","execution_count":null,"id":"0f348699-623c-401f-a946-d10cda9e0cf4","metadata":{"tags":[],"id":"0f348699-623c-401f-a946-d10cda9e0cf4"},"outputs":[],"source":["# Filtering out entries where the completion is 'PHI: NULL'\n","test_df = test_df[test_df['completion'] != 'PHI: NULL']\n","test_df = test_df.reset_index(drop=True)\n","# test_df"]},{"cell_type":"code","execution_count":null,"id":"6ae7299f-8844-4d9e-8239-00bcacb232a1","metadata":{"tags":[],"id":"6ae7299f-8844-4d9e-8239-00bcacb232a1"},"outputs":[],"source":["# Function to Split Multiple Answers in the 'Completion' Column into Separate Rows\n","def split_completions(row):\n","    \"\"\"\n","    Splits entries in the 'completion' column of a given row that contain multiple answers.\n","    Each answer is separated into a new row, maintaining other details from the original row.\n","    \"\"\"\n","    completions = row['completion'].split(' | ')\n","    new_rows = []\n","    for comp in completions:\n","        # Removes the 'END' tag from each completion before reappending it\n","        if 'END' in comp:\n","            comp = comp.replace(' END', '')\n","        new_row = row.copy()\n","        new_row['completion'] = comp + ' END'\n","        new_rows.append(new_row)\n","    return new_rows\n","\n","# Applying the function to each row and flattening the results into a new DataFrame\n","split_rows = [split_completions(row) for index, row in test_df.iterrows()]\n","flat_list = [item for sublist in split_rows for item in sublist]\n","new_df = pd.DataFrame(flat_list)"]},{"cell_type":"code","execution_count":null,"id":"ddd0bbba-e6aa-4e5e-89e8-d7bc0014cdb4","metadata":{"scrolled":true,"tags":[],"id":"ddd0bbba-e6aa-4e5e-89e8-d7bc0014cdb4","outputId":"47cace4a-f279-4628-e52a-e7e6e07572d3"},"outputs":[{"data":{"text/plain":["[{'prompt': 'Please extract HIPAA related information from the given text: file ID:file35016, start:0, content:SPR no: 10L407622M\\n\\n###\\n\\n',\n","  'completion': 'Type: IDNUM, Content: 10L407622M END'},\n"," {'prompt': 'Please extract HIPAA related information from the given text: file ID:file35016, start:0, content:SPR no: 10L407622M\\n\\n###\\n\\n',\n","  'completion': 'Type: IDNUM, Content: 10L40762 END'}]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Converting the DataFrame to a JSON format for submission\n","predictions_json = new_df.to_dict(orient='records')\n","predictions_json = [{\"prompt\": record[\"prompt\"], \"completion\": record[\"completion\"]} for record in predictions_json]\n","\n","# Show top n items for checking\n","predictions_json[0:2]"]},{"cell_type":"code","execution_count":null,"id":"0364ad84-2fc0-4ad6-bc84-dc46403174a1","metadata":{"scrolled":true,"tags":[],"id":"0364ad84-2fc0-4ad6-bc84-dc46403174a1","outputId":"7337f6c7-15b8-4951-8cbd-9eaed3336d84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: twice | Old normalized: R2 | New: Type: SET, Content: twice (Normalized: R2)\n","Original: twice | Old normalized: R2 | New: Type: SET, Content: twice (Normalized: R2)\n","Original: twice | Old normalized: R2 | New: Type: SET, Content: twice (Normalized: R2)\n"]}],"source":["def normalize_set(data):\n","    \"\"\"\n","    Normalizes specific patterns in the 'completion' field of each item in the provided dataset.\n","    It specifically targets and normalizes occurrences of 'once', 'twice', and 'three times'.\n","    \"\"\"\n","    normalization_map = {\n","        'once': 'R1',\n","        'twice': 'R2',\n","        'three times': 'R3',\n","    }\n","\n","    for item in data:\n","        set_strings = re.findall(r'Type: SET, Content: (.*?) \\(Normalized: (.*?)\\)', item[\"completion\"])\n","\n","        for set_str, old_normalized in set_strings:\n","            normalized_str = normalization_map.get(set_str, set_str)\n","            new_completion = f\"Type: SET, Content: {set_str} (Normalized: {normalized_str})\"\n","            item[\"completion\"] = item[\"completion\"].replace(\n","                f\"Type: SET, Content: {set_str} (Normalized: {old_normalized})\",\n","                new_completion\n","            )\n","            print(\"Original:\", set_str, \"| Old normalized:\", old_normalized, \"| New:\", new_completion)\n","\n","    return data\n","\n","normalized_set = normalize_set(predictions_json)"]},{"cell_type":"code","execution_count":null,"id":"3cf55394-c2ee-49af-afc9-f46052528297","metadata":{"scrolled":true,"tags":[],"id":"3cf55394-c2ee-49af-afc9-f46052528297","outputId":"3d2438ff-3532-449e-d37e-2104b3bdaa39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done\n"]}],"source":["def convert_to_iso(datetime_str):\n","    if pd.notna(datetime_str):\n","        formats = [\n","            \"%d/%m/%Y at %H:%Mhr\", \"%Y-%m-%d %H:%M:%S\", \"%d/%m/%Y %H:%M:%S\", \"%d/%m/%Y %H:%M\", \"%I:%M%p on %d/%m/%Y\",\n","            \"%I%M%p on %d.%m.%y\", \"%I.%M%p on %d.%m.%y\", \"%d/%m/%y on %I%p\", \"%I:%M%p on the %d.%m.%Y\", \"%I:%M%p on %d/%m/%y\",\n","            \"%d/%m/%Y at %H:%M\", \"%d.%m.%y at %I:%M %p\",  \"%I.%M%p on %d.%m.%y\", \"%d.%m.%y on %I:%M %p\", \"%I:%M%p on %d.%m.%y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%d/%m/%Y %I:%M %p\", \"%H:%M on %d/%m/%y\", \"%I:%M%p at %d/%m/%Y\", \"%H:%M on %m/%d/%Y\",\n","            \"%I:%M%p on %d.%m.%Y\", \"%I:%M%p on %d/%m/%y\", \"%d/%m/%y\", \"%H%M%hrs on %d.%m.%y\", \"%H:%M on %d/%m/%y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%I:%M%p on %m/%d/%Y\", \"%H:%M:%S on %d/%m/%Y\", \"%H:%M:%S on %m/%d/%Y\", \"%I:%M%p on %m/%d/%Y\",\n","            \"%H:%M on %m/%d/%Y\", \"%H:%M:%S on %d/%m/%Y\", \"%H:%M:%S on %m/%d/%Y\", \"%H:%M:%S on %d.%m.%Y\", \"%H:%M:%S on %m.%d.%Y\",\n","            \"%I:%M%p on %m.%d.%Y\", \"%H:%M:%S on %d/%m/%y\", \"%H:%M:%S on %m/%d/%y\", \"%I:%M%p on %m/%d/%y\", \"%H:%M:%S %p on %d/%m/%Y\",\n","            \"%H:%M:%S %p on %m/%d/%Y\", \"%I:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",  \"%I:%M:%S %p on %d/%m/%y\", \"%H:%M:%S %p on %m/%d/%y\",\n","            \"%I:%M:%S %p on %m/%d/%y\", \"%H:%M:%S %p on %d/%m/%Y\", \"%H:%M:%S %p on %m/%d/%Y\", \"%I:%M:%S %p on %d.%m.%Y\",\n","            \"%I:%M:%S %p on %m.%d.%Y\", \"%I:%M:%S %p on %d/%m/%y\", \"%I:%M:%S %p on %m/%d/%y\", \"%I:%M:%S %p on %d.%m.%y\",\n","            \"%I:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %d/%m/%Y\", \"%I:%M %p on %m/%d/%Y\", \"%I:%M %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d/%m/%y\", \"%I:%M %p on %m/%d/%y\", \"%I:%M %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\",\n","            \"%H:%M on %d.%m.%y\", \"%I:%M %p on %d/%m/%Y\", \"%I:%M %p on %m/%d/%Y\", \"%I:%M %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d/%m/%y\", \"%I:%M %p on %m/%d/%y\", \"%I:%M %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\", \"%H:%M:%S on %d.%m.%y\",\n","            \"%H:%M:%S %p on %d.%m.%y\", \"%H:%M:%S on %m.%d.%y\", \"%H:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %m/%d/%Y\",\n","            \"%H:%M:%S %p on %m/%d/%Y\", \"%I:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m/%d/%y\",\n","            \"%H:%M:%S %p on %m/%d/%y\", \"%I:%M %p on %d.%m.%y\", \"%H:%M:%S %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\",\n","            \"%H:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\",\n","            \"%H:%M:%S %p on %m.%d.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%H:%M:%S %p on %d.%m.%y\",\n","            \"%H:%M:%S %p on %m.%d.%y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%I:%M %p on %d.%m.%y\",\n","            \"%H:%M:%S %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\", \"%H:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %d.%m.%Y\",\n","            \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%I:%M %p on %d.%m.%Y\",\n","            \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%H:%M %p on %d.%m.%y\",\n","            \"%H:%M:%S %p on %d.%m.%y\", \"%H:%M:%S %p on %m.%d.%y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",\n","            \"%H:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d.%m.%y\", \"%H:%M:%S %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\", \"%H:%M:%S %p on %m.%d.%y\",\n","            \"%H%Mhrs on %d.%m.%y\", \"%H%Mhrs on %d.%m.%Y\", \"%H:%Mhrs on %d.%m.%Y\", \"%I:%M %p on the %d/%m/%y\",\n","            \"%d/%m/%Y\", \"%H.%M on %d/.%m.%y\", \"%d/%m/%Yat %H:%M\", \"%I.%M%p on %d/%m/%y\", \"%H.%M on %d.%m.%y\",\n","            \"%H%Mhr on %d/%m/%y\", \"%H.%M%p on %d.%m.%y\", \"%H.%M%p on %d/%m/%y\", \"%H.%M on %d/%m/%y\",\n","            \"%I%p on %d.%m.%y\", \"%H.%M o %d.%m.%y\", \"%I:%M%p on %d.%m.%y\", \"%H:%M%p on %d/%m/%y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%H:%M%p on %d.%m.%y\", \"%H:%M on the %dth of %B %Y\", \"%H%M on the %d/%m/%y\",\n","            \"%H:%M on %d.%m.%Y\", \"%d.%m.%Y at %I:%M%p\", \"%d/%m/%y at %I:%M%p\", \"%H%M%hrs on %D.%M.%Y\", \"%d.%m.%y at %I:%M%p\",\n","            \"%H%M on the %d of %B %Y\", \"%I:%M%p on the %d of %B %Y\", \"%d/%m/%y %I:%M %p\", \"%dth of %B %Y at %I:%M%p\",\n","            \"%H:%M on the %d/%m/%y\", \"%d/%m/%y at %I%p\", \"%H:%Mhrs on %d/%m/%y\", \"%I%M on the %dth of %B %Y\",\n","            \"%I:%M%p on the %dth of %B %Y\", \"%I:%M%p on the %dst of %B %Y\", \"%I:%M%p on the %dnd of %B %Y\", \"%I:%M%p on the %drd of %B %Y\",\n","            \"%H%M on the %dth of %B %Y\", \"%d.%m.%y\", \"%I:%M%p on %d/%m/%Y\", \"%I:%M%p on %d/%m/%Y\", \"%I.%M%p on %d/%m/%Y\", \"%I:%M%p on %d/%m/%y\",\n","            \"%I.%M%p on %d/%m/%y\", \"%H:%M%p on %d/%m/%Y\", \"%H.%M%p on %d/%m/%Y\", \"%H:%M%p on %d/%m/%y\", \"%H.%M%p on %d/%m/%y\",\n","            \"%I:%M%p on %m/%d/%Y\", \"%I.%M%p on %m/%d/%Y\", \"%I:%M%p on %m/%d/%y\", \"%I.%M%p on %m/%d/%y\", \"%H:%M%p on %m/%d/%Y\",\n","            \"%H.%M%p on %m/%d/%Y\", \"%H:%M%p on %m/%d/%y\", \"%H.%M%p on %m/%d/%y\", \"%I:%M%p on %d/%m/%Y\", \"%I.%M%p on %d/%m/%Y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%I.%M%p on %d/%m/%y\", \"%H:%M%p on %d/%m/%Y\", \"%H.%M%p on %d/%m/%Y\", \"%H:%M%p on %d/%m/%y\",\n","            \"%H.%M%p on %d/%m/%y\", \"%d.%m.%y at %I:%M %p\",\"%I.%M%p, %d/%m/%y\", \"%H:%M%p on %d.%m.%Y\", \"%I.%M %p on %d/%m/%y\",\n","            \"%d.%m.%Y at %I:%M%p\", \"%d/%m/%Y at %I:%M%p\", \"%d.%m.%Y at %I;%M %p\", \"%m/%d/%Y at %H:%M\", \"%I%M%p on %d/%m/%Y\",\n","            \"%I%p on %d/%m/%y\", \"%I%M%p on %d.%m.%y\", \"%I:%Mhrs %d/%m/%y\", \"%I%Mh on %d/%m/%Y\", \"%H:%M hrs on %d/%m/%y\", \"%H:%Mhr on %d/%m/%Y\",\n","            \"%I%Mhrs on %d/%m/%Y\", \"%HM on %d.%m.%y\", \"%H%M on %d.%m.%y\", \"%H:%M Hrs on %d.%m.%y\", \"%I%p on %d.%m.%Y\",\n","            \"%d/%m/%Y at %I:%M%p\", \"%I:%M hours on %d/%m/%y\", \"%I:%M%Hrs on %d.%m.%y\", \"%I.%M%p, %d/%m/%y\", \"%d.%m.%y %I%p\", \"%I.%Mhrs on %d.%m.%y\",\n","            \"%I.%Mhrs on %d.%m.%y\", \"%I:%Mhrs, %d/%m/%y\", \"%d/%m/%Y at%H:%M\", \"%H:%M %d/%m/%y\", \"%H:%M %d/%m/%y\", \"%I:%M%p on %d-%b-%Y\",\n","            \"%I:%Mhrs on %d/%m/%Y\", \"%I:%Mhrs on %d/%m/%Y\", \"%I:%Mhrs on %d-%b-%Y\", \"%I%Mhrs on %d/%m/%Y\", \"%I%p on %d/%m/%y\", \"%d.%m.%y %I.%Mhrs\",\n","            \"%d/%m/%y %I:%M%p\", \"%H%M%p on %d.%m.%y\", \"%I%M%p on %d.%m.%y\",\"%I%p on %d.%m.%y\", \"%I.%M%p on %d.%m.%y\",\n","            \"%H:%M %p on %d.%m.%y\", \"%d.%m.%Y at %I:%M %p\",\n","            \"%H%MH on %d.%m.%y\", \"%I%M%p on %d/%m/%Y\", \"%I%M%p on %d.%m.%y\", \"%H%Mh on %d/%m/%Y\", \"%H%Mhrs on %d/%m/%Y\", \"%H:%MH on %d/%m/%y\",\n","            \"%H:%MHrs on %d.%m.%y\", \"%H.%Mhrs on %d.%m.%y\", \"%H:%Mhr on %d/%m/%y\", \"%H%Mhrs on %d/%m/%Y\", \"%H.%M hrs on %d.%m.%y\",\n","            \"%d.%m.%Y at %I:%M %p\", \"%d.%m.%Y at %I:%M%p\", \"%d/%m/%Y at%H:%M\",\n","            \"%d.%m.%Y at %I.%M%p\", \"%I%M%p on %d/%m/%Y\", \"%m/%d/%Y at%H:%M\", \"%I:%M%p on the %d-%b-%Y\", \"%H:%Mhrs on the %d-%b-%Y\"\n","        ]\n","\n","        for fmt in formats:\n","            try:\n","                parsed_date_time = datetime.strptime(datetime_str, fmt)\n","                if \"%S\" in fmt:\n","                    return parsed_date_time.isoformat()\n","                else:\n","                    return parsed_date_time.strftime(\"%Y-%m-%dT%H:%M\")\n","            except ValueError:\n","                pass\n","\n","        return datetime_str\n","\n","def convert_to_desired_format(date_str):\n","    \"\"\"\n","    Converts various date string formats to a standardized YYYY-MM-DD format or YYYY format.\n","    It handles different separator characters and year, month, day order variations.\n","    \"\"\"\n","    regex_patterns = [\n","        r\"\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{2})\\b\",   # DD.MM.YY or MM/DD/YY\n","        r\"\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{4})\\b\",   # DD.MM.YYYY or MM/DD/YYYY\n","        r\"\\b(\\d{4})\\b\",                             # YYYY\n","        r\"\\b(\\d{4})(\\d{2})(\\d{2})\\b\",               # YYYYMMDD\n","        r\"\\b(\\d{1,2})[./](\\d{1,2})(\\d{4})\\b\",       # DDMMYYYY without separators\n","        r\"\\b(\\d{4})(\\d{2})(\\d{3})\\b\"                # YYYYMMDDD\n","    ]\n","    for pattern in regex_patterns:\n","        match = re.search(pattern, date_str)\n","        if match:\n","            groups = match.groups()\n","            if pattern == regex_patterns[0] or pattern == regex_patterns[1]:\n","                return f\"{groups[2]:0>4}-{groups[1]:0>2}-{groups[0]:0>2}\"\n","            elif pattern == regex_patterns[2]:\n","                return f\"{groups[0]:0>4}\"\n","            elif pattern == regex_patterns[3]:\n","                return f\"{groups[0]}-{groups[1]:0>2}-{groups[2]:0>2}\"\n","            elif pattern == regex_patterns[4]:\n","                return f\"{groups[2]:0>4}-{groups[1]:0>2}-{groups[0]:0>2}\"\n","            elif pattern == regex_patterns[5]:\n","                return f\"{groups[0]}-{groups[1]:0>2}-{groups[2][:2]}\"\n","    return date_str\n","\n","def replace_leading_zero(date_str):\n","    \"\"\"\n","    Replaces a leading zero in a date string with '2'.\n","    Useful for correcting certain date formats where the century might be incorrectly represented.\n","    \"\"\"\n","    if date_str.startswith(\"0\"):\n","        return \"2\" + date_str[1:]\n","    else:\n","        return date_str\n","\n","def process_date_time(data):\n","    \"\"\"\n","    Processes and normalizes date and time strings in the 'completion' field of each item in the dataset.\n","    Uses separate functions to convert date and time strings to standard formats.\n","    \"\"\"\n","    for item in data:\n","        # Find all date or time strings in the 'completion' field\n","        datetime_strings = re.findall(r'Type: (DATE|TIME), Content: (.*?) \\((Normalized: )?(.*?)\\)', item[\"completion\"])\n","\n","        for dtype, dt_str, _, old_normalized in datetime_strings:\n","            if dtype == \"TIME\":\n","                # Normalize time strings (implement convert_to_iso separately)\n","                normalized_str = convert_to_iso(dt_str)\n","            elif dtype == \"DATE\":\n","                # Normalize date strings and replace leading zero if necessary\n","                normalized_str = convert_to_desired_format(dt_str)\n","                normalized_str = replace_leading_zero(normalized_str)\n","            else:\n","                continue\n","\n","            new_completion = f\"Type: {dtype}, Content: {dt_str} (Normalized: {normalized_str})\"\n","            # Update the 'completion' field with the new normalized string\n","            item[\"completion\"] = item[\"completion\"].replace(\n","                f\"Type: {dtype}, Content: {dt_str} (Normalized: {old_normalized})\",\n","                new_completion\n","            )\n","\n","    return data\n","\n","normalized_date_time = process_date_time(normalized_set)\n","print('Done')"]},{"cell_type":"code","source":["def clean_and_reformat_content(predictions_json):\n","    \"\"\"\n","    Cleans and reformats the content from the predictions_json for submission.\n","    Extracts necessary information from each json entry and creates a tab-separated format.\n","    \"\"\"\n","    reformatted_content = []\n","\n","    for json_entry in predictions_json:\n","        # Search for file ID, start position, and content text in the prompt\n","        prompt_search = re.search(r'file ID:([a-zA-Z]*\\d+), start:(\\d+), content:(.+)\\n\\n###\\n\\n', json_entry['prompt'])\n","        if not prompt_search:\n","            continue\n","        file_id, start_pos, content_text = prompt_search.groups()\n","        start_pos = int(start_pos)\n","\n","        # Find all matches of the pattern in the 'completion' field\n","        matches = re.findall(r'Type: ([\\w-]+), Content: (.*?)(?:\\s+\\(Normalized: ([^\\)]+)\\))? END', json_entry['completion'])\n","        for match in matches:\n","            tag_type, text_info, normalized_date = match\n","            # Find the start index of the text_info in the content_text\n","            relative_start_index = content_text.find(text_info.strip())\n","            if relative_start_index == -1 and tag_type == \"TIME\":\n","                # Special handling for time type to match the format\n","                time_match = re.search(r'\\d{1,2}:\\d{2}\\w*m on \\d{1,2}\\.\\d{1,2}\\.\\d{2}', content_text)\n","                if time_match:\n","                    text_info = time_match.group()\n","                    relative_start_index = content_text.find(text_info.strip())\n","\n","            if relative_start_index != -1:\n","                # Calculate the actual start and end index\n","                actual_start_index = start_pos + relative_start_index\n","                actual_end_index = actual_start_index + len(text_info.strip())\n","\n","                # Create a reformatted line for each match\n","                reformatted_line = f\"{file_id}\\t{tag_type}\\t{actual_start_index}\\t{actual_end_index}\\t{text_info.strip()}\"\n","                if normalized_date:\n","                    reformatted_line += f\"\\t{normalized_date}\"\n","                reformatted_line += \"\\n\"\n","                reformatted_content.append(reformatted_line)\n","\n","    return reformatted_content\n","\n","reformatted_content = clean_and_reformat_content(normalized_date_time)"],"metadata":{"id":"PzN_kw_XWXf-"},"id":"PzN_kw_XWXf-","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7435656f-fd97-45d1-9960-f20fd99b6230","metadata":{"tags":[],"id":"7435656f-fd97-45d1-9960-f20fd99b6230"},"outputs":[],"source":["# Writing the reformatted content to a text file\n","txt_file_path = f\"./Result/{model_name}-{config['generation_max_length']}-answer.txt\"\n","\n","with open(txt_file_path, 'w') as file:\n","    for line in reformatted_content:\n","        file.write(line)"]},{"cell_type":"code","execution_count":null,"id":"7d2ce692-249b-4e69-a17b-f4c3f88d716a","metadata":{"id":"7d2ce692-249b-4e69-a17b-f4c3f88d716a"},"outputs":[],"source":["zip_file_path = f\"./Result/{model_name}-{config['generation_max_length']}-answer.txt.zip\"\n","\n","# Save zip file\n","with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n","    zipf.write(txt_file_path, arcname='answer.txt')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}