{"cells":[{"cell_type":"markdown","id":"2ad7bc3c-7c42-4b58-8f2a-33c0599a219c","metadata":{"id":"2ad7bc3c-7c42-4b58-8f2a-33c0599a219c"},"source":["# README\n","- 2023/11/30 添加Rule-base，使用規則的方式提取Type，包含COUNTRY, LOCATION-OTHER, PHONE, DURATION\n","    - Task 1 from 0.7787114 to 0.918699\n","    - Task 2 from 0.4870643 to 0.7900468\n","- 2023/12/02 添加DATE, TIME 正則化方法\n","    - Task 2 from 0.7900468 to 0.8313909"]},{"cell_type":"code","execution_count":null,"id":"bcc420a4-f0b0-4dfd-a330-8aacb1e44ca7","metadata":{"tags":[],"id":"bcc420a4-f0b0-4dfd-a330-8aacb1e44ca7"},"outputs":[],"source":["!pip install -q transformers datasets rouge_score evaluate accelerate scikit-learn pycountry"]},{"cell_type":"code","execution_count":null,"id":"6be12cac-09c0-4189-a413-b80c88ccd7a6","metadata":{"tags":[],"id":"6be12cac-09c0-4189-a413-b80c88ccd7a6"},"outputs":[],"source":["# Best model in validation\n","# fine_tune_model_dir = \"/mnt/nas/HYZ/AICUP/實驗紀錄/\"\n","# fine_tune_model_name = \"1701326984-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","# fine_tune_model_name = \"1700994394-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","\n","# Use Fine Tune on Train + Validarion model\n","fine_tune_model_dir = \"/mnt/nas/HYZ/AICUP/save_finetune_model/\"\n","\n","# # base on 1701326984 hyperparameter Exact Match 94.286300 16/2e-5/0.01/15\n","# fine_tune_model_name = \"1701406936-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","\n","# # base on 1700994394 hyperparameter, Exact Match 94.431500 4/2e-5/0.01/10\n","# fine_tune_model_name = \"1701407311-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","\n","# # base on 1701326984 hyperparameter, Exact Match 94.266900 16/2e-5/0.05/15\n","# fine_tune_model_name = \"1701489487-t5-efficient-base-dl2-finetuned-extracted-PHI\" # 沒上傳這個的成績！\n","\n","# # base on 1700994394 hyperparameter, Exact Match 94.567100, 4/2e-5/0.05/10\n","fine_tune_model_name = \"1701489459-t5-efficient-base-dl2-finetuned-extracted-PHI\"\n","config = {\n","    \"model_checkpoint\": f\"{fine_tune_model_dir}{fine_tune_model_name}\",\n","    \"max_input_length\": 512,\n","    \"max_target_length\": 100,\n","    \"batch_size\": 16,\n","    \"generation_max_length\": 100\n","}"]},{"cell_type":"code","execution_count":null,"id":"e2c17052-6020-48ec-9d6e-d99887e3e5f3","metadata":{"tags":[],"id":"e2c17052-6020-48ec-9d6e-d99887e3e5f3","outputId":"86b8dcb3-c53f-4874-d614-7011b67b3e8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n","\n","print(torch.cuda.device_count())"]},{"cell_type":"code","execution_count":null,"id":"98774647-4036-44e0-8ebf-bc586c0e0db4","metadata":{"tags":[],"id":"98774647-4036-44e0-8ebf-bc586c0e0db4","outputId":"31b209ac-d0ac-4cd8-8672-986e28267244"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2023-12-03 04:07:02,524] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"]}],"source":["from transformers import (\n","    AutoTokenizer, AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, GenerationConfig\n",")\n","from datasets import load_dataset, DatasetDict, Dataset, load_from_disk, concatenate_datasets\n","from tqdm import tqdm\n","from datetime import datetime\n","import pandas as pd\n","import numpy as np\n","import pycountry\n","import re\n","import json\n","import zipfile"]},{"cell_type":"code","execution_count":null,"id":"32859ae8-9215-4ff3-b0fa-df0cd9f71349","metadata":{"tags":[],"id":"32859ae8-9215-4ff3-b0fa-df0cd9f71349","outputId":"14b96dda-dbb4-4752-d118-888429ad103a"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['prompt', 'completion'],\n","        num_rows: 92933\n","    })\n","    validation: Dataset({\n","        features: ['prompt', 'completion'],\n","        num_rows: 10326\n","    })\n","    test: Dataset({\n","        features: ['prompt'],\n","        num_rows: 47084\n","    })\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Loading dataset\n","file_path = '/mnt/nas/HYZ/AICUP/'\n","# dataset = load_from_disk(f\"{file_path}dataset_dict\")\n","dataset = load_from_disk(f\"{file_path}dataset_dict_v2\")\n","\n","dataset"]},{"cell_type":"code","execution_count":null,"id":"ecbea832-8c01-42ff-90d8-f5c17db11bf5","metadata":{"tags":[],"id":"ecbea832-8c01-42ff-90d8-f5c17db11bf5"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(config[\"model_checkpoint\"])\n","model = AutoModelForSeq2SeqLM.from_pretrained(config[\"model_checkpoint\"])"]},{"cell_type":"code","execution_count":null,"id":"e3a226be-046b-43d8-afdf-578e8871828e","metadata":{"tags":[],"id":"e3a226be-046b-43d8-afdf-578e8871828e","outputId":"63a5e3cd-b173-4226-b7b7-1c40bb8b6b31","colab":{"referenced_widgets":["14c13e13ef014eb99d3f4bba18754e40","02e6fbfedf4f409193fbc263a64bd977","78f27d6e0cdd4b2ab27c3397cfd1c0a0"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14c13e13ef014eb99d3f4bba18754e40","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/92933 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02e6fbfedf4f409193fbc263a64bd977","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10326 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78f27d6e0cdd4b2ab27c3397cfd1c0a0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/47084 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Keys of tokenized dataset: ['prompt', 'input_ids', 'attention_mask']\n"]}],"source":["# Function to preprocess the data for the T5 model\n","def preprocess_function(examples):\n","    # Tokenizing the input\n","    model_inputs = tokenizer(examples[\"prompt\"], padding=\"max_length\", max_length=config[\"max_input_length\"], truncation=True)\n","\n","    if \"completion\" in examples:\n","        # Tokenizing the labels (if they exist) and setting padding tokens to -100\n","        labels = tokenizer(examples[\"completion\"], padding=\"max_length\", max_length=config[\"max_target_length\"], truncation=True)\n","        labels[\"input_ids\"] = [\n","            [(label if label != tokenizer.pad_token_id else -100) for label in label_example] for label_example in labels[\"input_ids\"]\n","        ]\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs\n","\n","# Applying the preprocessing function to the datasets\n","tokenized_dataset = dataset.map(preprocess_function, batched=True)\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset['test'].features)}\")"]},{"cell_type":"code","execution_count":null,"id":"7c2f4d5f-109b-4e60-aa73-fe95b7be2580","metadata":{"tags":[],"id":"7c2f4d5f-109b-4e60-aa73-fe95b7be2580","outputId":"78ce0909-2cdf-49ff-db22-13741bc22837"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model List: ['1701489459', 't5', 'efficient', 'base', 'dl2', 'finetuned', 'extracted', 'PHI']\n","Model Name: 1701489459\n"]}],"source":["# Setting up training arguments for fine-tuning\n","model_list = config[\"model_checkpoint\"].split(\"/\")[-1].split(\"-\")\n","model_name = model_list[0]\n","print(f\"Model List: {model_list}\")\n","print(f\"Model Name: {model_name}\")"]},{"cell_type":"code","execution_count":null,"id":"16e009fa-dc85-47c0-917f-f9b9bb3fa20e","metadata":{"tags":[],"id":"16e009fa-dc85-47c0-917f-f9b9bb3fa20e","outputId":"b7f96d8e-3826-4362-f015-e11f0bd9339f"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=Seq2SeqTrainingArguments(\n","        # output_dir=\"finetune-model-predicted\",\n","        output_dir=\"Result\",\n","        per_device_eval_batch_size=config[\"batch_size\"]*config[\"batch_size\"],\n","        predict_with_generate=True,\n","        generation_max_length=config[\"generation_max_length\"],\n","    ),\n","    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, pad_to_multiple_of=8),\n","    tokenizer=tokenizer,\n",")\n","\n","test_predictions = trainer.predict(tokenized_dataset[\"test\"])"]},{"cell_type":"code","execution_count":null,"id":"f196e0d8-e787-4f6d-a347-119ce2fbd372","metadata":{"tags":[],"id":"f196e0d8-e787-4f6d-a347-119ce2fbd372","outputId":"11bd8b27-a82c-4880-8807-d070a9a37a0b"},"outputs":[{"data":{"text/plain":["array([    0,  6632,    10,    27, 12145,  6122,     6,  7185,    10,\n","         335,   434,  2445,  3959,  2884,   329,  1820,  6632,    10,\n","          27, 12145,  6122,     6,  7185,    10,   335,   434,  2445,\n","        3959,   357,     3, 14920,     1,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["test_predictions[0][0]"]},{"cell_type":"code","execution_count":null,"id":"1b5f749a-0fc1-47af-9c27-207c29c8ae9a","metadata":{"tags":[],"id":"1b5f749a-0fc1-47af-9c27-207c29c8ae9a","outputId":"a6b9df7b-01de-4c4c-cb83-17f04aa77ae9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Type: IDNUM, Content: 10L407622M | Type: IDNUM, Content: 10L40762 END\n"]}],"source":["# Decoding and filtering the test predictions\n","decoded_predictions = []\n","for pred in test_predictions.predictions:\n","    filtered_pred = [token_id for token_id in pred if token_id != -100 and 0 <= token_id < tokenizer.vocab_size]\n","    decoded_text = tokenizer.decode(filtered_pred, skip_special_tokens=True)\n","    decoded_predictions.append(decoded_text)\n","\n","print(decoded_predictions[0])"]},{"cell_type":"markdown","id":"2e570617-7d2f-4866-8967-080d3ac752be","metadata":{"tags":[],"id":"2e570617-7d2f-4866-8967-080d3ac752be"},"source":["## Save Result"]},{"cell_type":"code","execution_count":null,"id":"5128c098-dad0-4e3c-85f0-4c69266fb40e","metadata":{"tags":[],"id":"5128c098-dad0-4e3c-85f0-4c69266fb40e"},"outputs":[],"source":["# Creating a DataFrame with prompts and their decoded completions\n","test_df = pd.DataFrame({\n","    \"prompt\": tokenized_dataset[\"test\"][\"prompt\"],\n","    \"completion\": decoded_predictions\n","})\n","\n","# test_df"]},{"cell_type":"code","execution_count":null,"id":"3ab426b0-39d3-45b8-b6a7-dba81c96c6fb","metadata":{"id":"3ab426b0-39d3-45b8-b6a7-dba81c96c6fb"},"outputs":[],"source":["# Save jsonl\n","output_file = f\"./Result/{model_name}-{config['generation_max_length']}-original_predicted.jsonl\"\n","test_df.to_json(output_file, orient=\"records\", lines=True)"]},{"cell_type":"markdown","id":"575e25d3-89e4-47bf-9d0f-742d29554524","metadata":{"id":"575e25d3-89e4-47bf-9d0f-742d29554524"},"source":["# Rule-base"]},{"cell_type":"code","execution_count":null,"id":"8b19c92c-eb1a-4263-a0ed-68b4d6985ee5","metadata":{"id":"8b19c92c-eb1a-4263-a0ed-68b4d6985ee5","outputId":"bea61c02-c75b-4814-adb2-c305ac1da083"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 47084/47084 [00:02<00:00, 21619.40it/s]\n","100%|██████████| 47084/47084 [00:03<00:00, 12598.18it/s]\n","100%|██████████| 47084/47084 [00:02<00:00, 20216.91it/s]\n","100%|██████████| 47084/47084 [00:02<00:00, 18242.52it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prompt</th>\n","      <th>completion</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>Type: IDNUM, Content: 10L407622M | Type: IDNUM...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>Type: MEDICALRECORD, Content: 10195149 END</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>Type: HOSPITAL, Content: COLAC AREA HEALTH END</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>47079</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47080</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47081</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47082</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","    <tr>\n","      <th>47083</th>\n","      <td>Please extract HIPAA related information from ...</td>\n","      <td>PHI: NULL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>47084 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  prompt  \\\n","0      Please extract HIPAA related information from ...   \n","1      Please extract HIPAA related information from ...   \n","2      Please extract HIPAA related information from ...   \n","3      Please extract HIPAA related information from ...   \n","4      Please extract HIPAA related information from ...   \n","...                                                  ...   \n","47079  Please extract HIPAA related information from ...   \n","47080  Please extract HIPAA related information from ...   \n","47081  Please extract HIPAA related information from ...   \n","47082  Please extract HIPAA related information from ...   \n","47083  Please extract HIPAA related information from ...   \n","\n","                                              completion  \n","0      Type: IDNUM, Content: 10L407622M | Type: IDNUM...  \n","1             Type: MEDICALRECORD, Content: 10195149 END  \n","2         Type: HOSPITAL, Content: COLAC AREA HEALTH END  \n","3                                              PHI: NULL  \n","4                                              PHI: NULL  \n","...                                                  ...  \n","47079                                          PHI: NULL  \n","47080                                          PHI: NULL  \n","47081                                          PHI: NULL  \n","47082                                          PHI: NULL  \n","47083                                          PHI: NULL  \n","\n","[47084 rows x 2 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# 子函數：提取手機號碼\n","def extracted_phone(row):\n","    phone_pattern = r'\\b\\d{4} \\d{4}\\b'\n","    phones = re.findall(phone_pattern, row['prompt'])\n","    if phones:\n","        row['completion'] = f\"Type: PHONE, Content: {phones[0]} END\"\n","    return row\n","\n","# 子函數：提取國家名稱\n","def extracted_country(row, countries, exceptions):\n","    # 檢查整個字串中是否含有 \"Dr\"、\"DR\" 或 \"PRO\"\n","    if re.search(r'\\b(Dr|DR|PRO)\\b', row['prompt']):\n","        # 進一步檢查 \"Dr\"、\"DR\" 或 \"PRO\" 後面的單詞是否為國家名\n","        dr_or_pro_match = re.search(r'\\b(Dr|DR|PRO)\\s+(\\w+)', row['prompt'])\n","        if dr_or_pro_match and dr_or_pro_match.group(2) in countries:\n","            return row  # 如果匹配到國家名，則跳過\n","\n","    # 檢查是否包含 \"with\" 並跟隨國家名稱\n","    if re.search(r'\\bwith\\s+(\\w+)', row['prompt']):\n","        with_match = re.search(r'\\bwith\\s+(\\w+)', row['prompt'])\n","        if with_match and with_match.group(1) in countries:\n","            if re.search(r'\\bwith\\s+\\w+\\s+\\(\\w+\\)', row['prompt']):\n","                return row\n","\n","    # 檢查例外情況\n","    if any(exception in row['prompt'] for exception in exceptions):\n","        return row\n","\n","    # 檢查國家名稱\n","    if re.search(r'\\bwith\\s+(\\w+)(?!\\s+\\()', row['prompt']):\n","        with_match = re.search(r'\\bwith\\s+(\\w+)(?!\\s+\\()', row['prompt'])\n","        if with_match and with_match.group(1) in countries:\n","            return row\n","    return row\n","\n","# 子函數：提取其他位置\n","def extracted_location_other(row):\n","    po_box_pattern = r'\\bP\\.?\\s*O\\.?\\s*BOX\\s+\\d+\\b'\n","    janborwill_pattern = r'\\bJANBORWILL\\b'\n","    po_box_matches = re.findall(po_box_pattern, row['prompt'], re.IGNORECASE)\n","    janborwill_matches = re.findall(janborwill_pattern, row['prompt'], re.IGNORECASE)\n","\n","    contents = po_box_matches + janborwill_matches\n","    if contents:\n","        row['completion'] = f\"Type: LOCATION-OTHER, Content: {', '.join(contents)} END\"\n","\n","    return row\n","\n","# 子函數：提取和標準化持續時間\n","def extract_and_normalized_duration(row):\n","    word_to_number = {'one': '1', 'two': '2', 'three': '3', 'four': '4',\n","                      'twenty': '20', 'forty': '40'}\n","\n","    duration_pattern = r'\\b((\\d+(-\\d+)?|one|two|three|twenty|four)\\s+(months?|years?|yrs?|weeks?|wks?|days?)|(\\d+)(?<!\\s)(yr|year))(?!\\s*(old|\\s*-|\\s*cycle|\\s*cycles))\\b'\n","    duration_matches = re.findall(duration_pattern, row['prompt'], re.IGNORECASE)\n","\n","    normalized_durations = []\n","    for match in duration_matches:\n","        text_num = match[1] if match[1] else match[4]\n","        unit_text = match[3] if match[3] else 'year'\n","\n","        # 转换文字描述的数量为数字\n","        num = word_to_number.get(text_num.lower(), text_num)\n","        unit = unit_text[0].upper()\n","\n","        normalized = \"P\" + num + unit\n","        duration_format = f\"{text_num} {unit_text}\"\n","        normalized_durations.append(f\"{duration_format} (Normalized: {normalized})\")\n","\n","    if normalized_durations:\n","        row['completion'] = f\"Type: DURATION, Content: {', '.join(normalized_durations)} END\"\n","\n","    return row\n","\n","\n","# 統合函數：提取信息\n","def extract_info(df):\n","    countries = [country.name for country in pycountry.countries]\n","    countries.sort(key=len, reverse=True)\n","    countries.append(\"Vietnam\") # pycountry is Viet Nam\n","    countries.append(\"USA\")\n","    exceptions = [\"Western Australia\", \"South Australia\", \"Congo red\", \"Congo Red\"]\n","\n","    tqdm.pandas()\n","\n","    for func in [extracted_phone, lambda row: extracted_country(row, countries, exceptions), extracted_location_other, extract_and_normalized_duration]:\n","        df = df.progress_apply(func, axis=1)\n","    return df\n","\n","# 應用函數\n","test_df = extract_info(test_df)\n","test_df"]},{"cell_type":"code","execution_count":null,"id":"5e98745b-7a2e-4d68-9b6f-ae5de10087c6","metadata":{"tags":[],"id":"5e98745b-7a2e-4d68-9b6f-ae5de10087c6"},"outputs":[],"source":["# Save jsonl\n","output_file = f\"./Result/{model_name}-{config['generation_max_length']}-after_rule_predicted.jsonl\"\n","test_df.to_json(output_file, orient=\"records\", lines=True)"]},{"cell_type":"code","execution_count":null,"id":"0f348699-623c-401f-a946-d10cda9e0cf4","metadata":{"tags":[],"id":"0f348699-623c-401f-a946-d10cda9e0cf4"},"outputs":[],"source":["# Filtering out entries where the completion is 'PHI: NULL'\n","test_df = test_df[test_df['completion'] != 'PHI: NULL']\n","test_df = test_df.reset_index(drop=True)\n","# test_df"]},{"cell_type":"code","execution_count":null,"id":"6ae7299f-8844-4d9e-8239-00bcacb232a1","metadata":{"tags":[],"id":"6ae7299f-8844-4d9e-8239-00bcacb232a1"},"outputs":[],"source":["# Function to split multiple answers in the 'completion' column into separate rows\n","def split_completions(row):\n","    completions = row['completion'].split(' | ')\n","    new_rows = []\n","    for comp in completions:\n","        if 'END' in comp:\n","            comp = comp.replace(' END', '')\n","        new_row = row.copy()\n","        new_row['completion'] = comp + ' END'\n","        new_rows.append(new_row)\n","    return new_rows\n","\n","# Applying the function to each row and flattening the results into a new DataFrame\n","split_rows = [split_completions(row) for index, row in test_df.iterrows()]\n","flat_list = [item for sublist in split_rows for item in sublist]\n","new_df = pd.DataFrame(flat_list)"]},{"cell_type":"code","execution_count":null,"id":"ddd0bbba-e6aa-4e5e-89e8-d7bc0014cdb4","metadata":{"scrolled":true,"tags":[],"id":"ddd0bbba-e6aa-4e5e-89e8-d7bc0014cdb4","outputId":"47cace4a-f279-4628-e52a-e7e6e07572d3"},"outputs":[{"data":{"text/plain":["[{'prompt': 'Please extract HIPAA related information from the given text: file ID:file35016, start:0, content:SPR no: 10L407622M\\n\\n###\\n\\n',\n","  'completion': 'Type: IDNUM, Content: 10L407622M END'},\n"," {'prompt': 'Please extract HIPAA related information from the given text: file ID:file35016, start:0, content:SPR no: 10L407622M\\n\\n###\\n\\n',\n","  'completion': 'Type: IDNUM, Content: 10L40762 END'}]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Converting the DataFrame to a JSON format for submission\n","predictions_json = new_df.to_dict(orient='records')\n","predictions_json = [{\"prompt\": record[\"prompt\"], \"completion\": record[\"completion\"]} for record in predictions_json]\n","\n","# Show top n items for checking\n","predictions_json[0:2]"]},{"cell_type":"code","execution_count":null,"id":"0364ad84-2fc0-4ad6-bc84-dc46403174a1","metadata":{"scrolled":true,"tags":[],"id":"0364ad84-2fc0-4ad6-bc84-dc46403174a1","outputId":"7337f6c7-15b8-4951-8cbd-9eaed3336d84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: twice | Old normalized: R2 | New: Type: SET, Content: twice (Normalized: R2)\n","Original: twice | Old normalized: R2 | New: Type: SET, Content: twice (Normalized: R2)\n","Original: twice | Old normalized: R2 | New: Type: SET, Content: twice (Normalized: R2)\n"]}],"source":["def normalize_set(data):\n","    normalization_map = {\n","        'once': 'R1',\n","        'twice': 'R2',\n","        'three times': 'R3',\n","    }\n","\n","    for item in data:\n","        # 查找并匹配 \"Type: SET, Content: ...\" 格式\n","        set_strings = re.findall(r'Type: SET, Content: (.*?) \\(Normalized: (.*?)\\)', item[\"completion\"])\n","\n","        for set_str, old_normalized in set_strings:\n","            # 查找对应的正则化字符串\n","            normalized_str = normalization_map.get(set_str, set_str)\n","            # 替换原字符串\n","            new_completion = f\"Type: SET, Content: {set_str} (Normalized: {normalized_str})\"\n","            item[\"completion\"] = item[\"completion\"].replace(\n","                f\"Type: SET, Content: {set_str} (Normalized: {old_normalized})\",\n","                new_completion\n","            )\n","            print(\"Original:\", set_str, \"| Old normalized:\", old_normalized, \"| New:\", new_completion)\n","\n","    return data\n","\n","normalized_set = normalize_set(predictions_json)"]},{"cell_type":"code","execution_count":null,"id":"3cf55394-c2ee-49af-afc9-f46052528297","metadata":{"scrolled":true,"tags":[],"id":"3cf55394-c2ee-49af-afc9-f46052528297","outputId":"3d2438ff-3532-449e-d37e-2104b3bdaa39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Done\n"]}],"source":["def convert_to_iso(datetime_str):\n","    if pd.notna(datetime_str):\n","        formats = [\n","            \"%d/%m/%Y at %H:%Mhr\", \"%Y-%m-%d %H:%M:%S\", \"%d/%m/%Y %H:%M:%S\", \"%d/%m/%Y %H:%M\", \"%I:%M%p on %d/%m/%Y\",\n","            \"%I%M%p on %d.%m.%y\", \"%I.%M%p on %d.%m.%y\", \"%d/%m/%y on %I%p\", \"%I:%M%p on the %d.%m.%Y\", \"%I:%M%p on %d/%m/%y\",\n","            \"%d/%m/%Y at %H:%M\", \"%d.%m.%y at %I:%M %p\",  \"%I.%M%p on %d.%m.%y\", \"%d.%m.%y on %I:%M %p\", \"%I:%M%p on %d.%m.%y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%d/%m/%Y %I:%M %p\", \"%H:%M on %d/%m/%y\", \"%I:%M%p at %d/%m/%Y\", \"%H:%M on %m/%d/%Y\",\n","            \"%I:%M%p on %d.%m.%Y\", \"%I:%M%p on %d/%m/%y\", \"%d/%m/%y\", \"%H%M%hrs on %d.%m.%y\", \"%H:%M on %d/%m/%y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%I:%M%p on %m/%d/%Y\", \"%H:%M:%S on %d/%m/%Y\", \"%H:%M:%S on %m/%d/%Y\", \"%I:%M%p on %m/%d/%Y\",\n","            \"%H:%M on %m/%d/%Y\", \"%H:%M:%S on %d/%m/%Y\", \"%H:%M:%S on %m/%d/%Y\", \"%H:%M:%S on %d.%m.%Y\", \"%H:%M:%S on %m.%d.%Y\",\n","            \"%I:%M%p on %m.%d.%Y\", \"%H:%M:%S on %d/%m/%y\", \"%H:%M:%S on %m/%d/%y\", \"%I:%M%p on %m/%d/%y\", \"%H:%M:%S %p on %d/%m/%Y\",\n","            \"%H:%M:%S %p on %m/%d/%Y\", \"%I:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",  \"%I:%M:%S %p on %d/%m/%y\", \"%H:%M:%S %p on %m/%d/%y\",\n","            \"%I:%M:%S %p on %m/%d/%y\", \"%H:%M:%S %p on %d/%m/%Y\", \"%H:%M:%S %p on %m/%d/%Y\", \"%I:%M:%S %p on %d.%m.%Y\",\n","            \"%I:%M:%S %p on %m.%d.%Y\", \"%I:%M:%S %p on %d/%m/%y\", \"%I:%M:%S %p on %m/%d/%y\", \"%I:%M:%S %p on %d.%m.%y\",\n","            \"%I:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %d/%m/%Y\", \"%I:%M %p on %m/%d/%Y\", \"%I:%M %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d/%m/%y\", \"%I:%M %p on %m/%d/%y\", \"%I:%M %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\",\n","            \"%H:%M on %d.%m.%y\", \"%I:%M %p on %d/%m/%Y\", \"%I:%M %p on %m/%d/%Y\", \"%I:%M %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d/%m/%y\", \"%I:%M %p on %m/%d/%y\", \"%I:%M %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\", \"%H:%M:%S on %d.%m.%y\",\n","            \"%H:%M:%S %p on %d.%m.%y\", \"%H:%M:%S on %m.%d.%y\", \"%H:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %m/%d/%Y\",\n","            \"%H:%M:%S %p on %m/%d/%Y\", \"%I:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m/%d/%y\",\n","            \"%H:%M:%S %p on %m/%d/%y\", \"%I:%M %p on %d.%m.%y\", \"%H:%M:%S %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\",\n","            \"%H:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\",\n","            \"%H:%M:%S %p on %m.%d.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%H:%M:%S %p on %d.%m.%y\",\n","            \"%H:%M:%S %p on %m.%d.%y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%I:%M %p on %d.%m.%y\",\n","            \"%H:%M:%S %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\", \"%H:%M:%S %p on %m.%d.%y\", \"%I:%M %p on %d.%m.%Y\",\n","            \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%I:%M %p on %d.%m.%Y\",\n","            \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\", \"%H:%M %p on %d.%m.%y\",\n","            \"%H:%M:%S %p on %d.%m.%y\", \"%H:%M:%S %p on %m.%d.%y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",\n","            \"%H:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%H:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d.%m.%Y\", \"%H:%M:%S %p on %d.%m.%Y\", \"%I:%M %p on %m.%d.%Y\", \"%H:%M:%S %p on %m.%d.%Y\",\n","            \"%I:%M %p on %d.%m.%y\", \"%H:%M:%S %p on %d.%m.%y\", \"%I:%M %p on %m.%d.%y\", \"%H:%M:%S %p on %m.%d.%y\",\n","            \"%H%Mhrs on %d.%m.%y\", \"%H%Mhrs on %d.%m.%Y\", \"%H:%Mhrs on %d.%m.%Y\", \"%I:%M %p on the %d/%m/%y\",\n","            \"%d/%m/%Y\", \"%H.%M on %d/.%m.%y\", \"%d/%m/%Yat %H:%M\", \"%I.%M%p on %d/%m/%y\", \"%H.%M on %d.%m.%y\",\n","            \"%H%Mhr on %d/%m/%y\", \"%H.%M%p on %d.%m.%y\", \"%H.%M%p on %d/%m/%y\", \"%H.%M on %d/%m/%y\",\n","            \"%I%p on %d.%m.%y\", \"%H.%M o %d.%m.%y\", \"%I:%M%p on %d.%m.%y\", \"%H:%M%p on %d/%m/%y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%H:%M%p on %d.%m.%y\", \"%H:%M on the %dth of %B %Y\", \"%H%M on the %d/%m/%y\",\n","            \"%H:%M on %d.%m.%Y\", \"%d.%m.%Y at %I:%M%p\", \"%d/%m/%y at %I:%M%p\", \"%H%M%hrs on %D.%M.%Y\", \"%d.%m.%y at %I:%M%p\",\n","            \"%H%M on the %d of %B %Y\", \"%I:%M%p on the %d of %B %Y\", \"%d/%m/%y %I:%M %p\", \"%dth of %B %Y at %I:%M%p\",\n","            \"%H:%M on the %d/%m/%y\", \"%d/%m/%y at %I%p\", \"%H:%Mhrs on %d/%m/%y\", \"%I%M on the %dth of %B %Y\",\n","            \"%I:%M%p on the %dth of %B %Y\", \"%I:%M%p on the %dst of %B %Y\", \"%I:%M%p on the %dnd of %B %Y\", \"%I:%M%p on the %drd of %B %Y\",\n","            \"%H%M on the %dth of %B %Y\", \"%d.%m.%y\", \"%I:%M%p on %d/%m/%Y\", \"%I:%M%p on %d/%m/%Y\", \"%I.%M%p on %d/%m/%Y\", \"%I:%M%p on %d/%m/%y\",\n","            \"%I.%M%p on %d/%m/%y\", \"%H:%M%p on %d/%m/%Y\", \"%H.%M%p on %d/%m/%Y\", \"%H:%M%p on %d/%m/%y\", \"%H.%M%p on %d/%m/%y\",\n","            \"%I:%M%p on %m/%d/%Y\", \"%I.%M%p on %m/%d/%Y\", \"%I:%M%p on %m/%d/%y\", \"%I.%M%p on %m/%d/%y\", \"%H:%M%p on %m/%d/%Y\",\n","            \"%H.%M%p on %m/%d/%Y\", \"%H:%M%p on %m/%d/%y\", \"%H.%M%p on %m/%d/%y\", \"%I:%M%p on %d/%m/%Y\", \"%I.%M%p on %d/%m/%Y\",\n","            \"%I:%M%p on %d/%m/%y\", \"%I.%M%p on %d/%m/%y\", \"%H:%M%p on %d/%m/%Y\", \"%H.%M%p on %d/%m/%Y\", \"%H:%M%p on %d/%m/%y\",\n","            \"%H.%M%p on %d/%m/%y\", \"%d.%m.%y at %I:%M %p\",\"%I.%M%p, %d/%m/%y\", \"%H:%M%p on %d.%m.%Y\", \"%I.%M %p on %d/%m/%y\",\n","            \"%d.%m.%Y at %I:%M%p\", \"%d/%m/%Y at %I:%M%p\", \"%d.%m.%Y at %I;%M %p\", \"%m/%d/%Y at %H:%M\", \"%I%M%p on %d/%m/%Y\",\n","            \"%I%p on %d/%m/%y\", \"%I%M%p on %d.%m.%y\", \"%I:%Mhrs %d/%m/%y\", \"%I%Mh on %d/%m/%Y\", \"%H:%M hrs on %d/%m/%y\", \"%H:%Mhr on %d/%m/%Y\",\n","            \"%I%Mhrs on %d/%m/%Y\", \"%HM on %d.%m.%y\", \"%H%M on %d.%m.%y\", \"%H:%M Hrs on %d.%m.%y\", \"%I%p on %d.%m.%Y\",\n","            \"%d/%m/%Y at %I:%M%p\", \"%I:%M hours on %d/%m/%y\", \"%I:%M%Hrs on %d.%m.%y\", \"%I.%M%p, %d/%m/%y\", \"%d.%m.%y %I%p\", \"%I.%Mhrs on %d.%m.%y\",\n","            \"%I.%Mhrs on %d.%m.%y\", \"%I:%Mhrs, %d/%m/%y\", \"%d/%m/%Y at%H:%M\", \"%H:%M %d/%m/%y\", \"%H:%M %d/%m/%y\", \"%I:%M%p on %d-%b-%Y\",\n","            \"%I:%Mhrs on %d/%m/%Y\", \"%I:%Mhrs on %d/%m/%Y\", \"%I:%Mhrs on %d-%b-%Y\", \"%I%Mhrs on %d/%m/%Y\", \"%I%p on %d/%m/%y\", \"%d.%m.%y %I.%Mhrs\",\n","            \"%d/%m/%y %I:%M%p\", \"%H%M%p on %d.%m.%y\", \"%I%M%p on %d.%m.%y\",\"%I%p on %d.%m.%y\", \"%I.%M%p on %d.%m.%y\",\n","            \"%H:%M %p on %d.%m.%y\", \"%d.%m.%Y at %I:%M %p\",\n","            \"%H%MH on %d.%m.%y\", \"%I%M%p on %d/%m/%Y\", \"%I%M%p on %d.%m.%y\", \"%H%Mh on %d/%m/%Y\", \"%H%Mhrs on %d/%m/%Y\", \"%H:%MH on %d/%m/%y\",\n","            \"%H:%MHrs on %d.%m.%y\", \"%H.%Mhrs on %d.%m.%y\", \"%H:%Mhr on %d/%m/%y\", \"%H%Mhrs on %d/%m/%Y\", \"%H.%M hrs on %d.%m.%y\",\n","            \"%d.%m.%Y at %I:%M %p\", \"%d.%m.%Y at %I:%M%p\", \"%d/%m/%Y at%H:%M\",\n","            \"%d.%m.%Y at %I.%M%p\", \"%I%M%p on %d/%m/%Y\", \"%m/%d/%Y at%H:%M\", \"%I:%M%p on the %d-%b-%Y\", \"%H:%Mhrs on the %d-%b-%Y\"\n","        ]\n","\n","        for fmt in formats:\n","            try:\n","                parsed_date_time = datetime.strptime(datetime_str, fmt)\n","                if \"%S\" in fmt:\n","                    return parsed_date_time.isoformat()\n","                else:\n","                    return parsed_date_time.strftime(\"%Y-%m-%dT%H:%M\")\n","            except ValueError:\n","                pass\n","\n","        return datetime_str\n","\n","def convert_to_desired_format(date_str):\n","    regex_patterns = [\n","        r\"\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{2})\\b\",\n","        r\"\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{4})\\b\",\n","        r\"\\b(\\d{4})\\b\",\n","        r\"\\b(\\d{4})(\\d{2})(\\d{2})\\b\",\n","        r\"\\b(\\d{1,2})[./](\\d{1,2})(\\d{4})\\b\",\n","        r\"\\b(\\d{4})(\\d{2})(\\d{3})\\b\"\n","    ]\n","    for pattern in regex_patterns:\n","        match = re.search(pattern, date_str)\n","        if match:\n","            groups = match.groups()\n","            if pattern == regex_patterns[0] or pattern == regex_patterns[1]:\n","                return f\"{groups[2]:0>4}-{groups[1]:0>2}-{groups[0]:0>2}\"\n","            elif pattern == regex_patterns[2]:\n","                return f\"{groups[0]:0>4}\"\n","            elif pattern == regex_patterns[3]:\n","                return f\"{groups[0]}-{groups[1]:0>2}-{groups[2]:0>2}\"\n","            elif pattern == regex_patterns[4]:\n","                return f\"{groups[2]:0>4}-{groups[1]:0>2}-{groups[0]:0>2}\"\n","            elif pattern == regex_patterns[5]:\n","                return f\"{groups[0]}-{groups[1]:0>2}-{groups[2][:2]}\"\n","    return date_str\n","\n","def replace_leading_zero(date_str):\n","    if date_str.startswith(\"0\"):\n","        return \"2\" + date_str[1:]\n","    else:\n","        return date_str\n","\n","def process_date_time(data):\n","    for item in data:\n","        datetime_strings = re.findall(r'Type: (DATE|TIME), Content: (.*?) \\((Normalized: )?(.*?)\\)', item[\"completion\"])\n","\n","        for dtype, dt_str, _, old_normalized in datetime_strings:\n","            if dtype == \"TIME\":\n","                normalized_str = convert_to_iso(dt_str)\n","            elif dtype == \"DATE\":\n","                normalized_str = convert_to_desired_format(dt_str)\n","                normalized_str = replace_leading_zero(normalized_str)\n","            else:\n","                continue\n","\n","            new_completion = f\"Type: {dtype}, Content: {dt_str} (Normalized: {normalized_str})\"\n","            # print(f\"Original: {dt_str} | Old normalized: {old_normalized} | New: {new_completion}\")\n","\n","            item[\"completion\"] = item[\"completion\"].replace(\n","                f\"Type: {dtype}, Content: {dt_str} (Normalized: {old_normalized})\",\n","                new_completion\n","            )\n","\n","    return data\n","\n","normalized_date_time = process_date_time(normalized_set)\n","print('Done')"]},{"cell_type":"code","execution_count":null,"id":"a4d48269-30c0-488e-b37b-121ddd873a5b","metadata":{"tags":[],"id":"a4d48269-30c0-488e-b37b-121ddd873a5b"},"outputs":[],"source":["# Function to clean and reformat content for submission\n","def clean_and_reformat_content(predictions_json):\n","    reformatted_content = []\n","\n","    for json_entry in predictions_json:\n","        prompt_search = re.search(r'file ID:([a-zA-Z]*\\d+), start:(\\d+), content:(.+)\\n\\n###\\n\\n', json_entry['prompt'])\n","        if not prompt_search:\n","            continue\n","        file_id, start_pos, content_text = prompt_search.groups()\n","        start_pos = int(start_pos)\n","\n","        matches = re.findall(r'Type: ([\\w-]+), Content: (.*?)(?:\\s+\\(Normalized: ([^\\)]+)\\))? END', json_entry['completion'])\n","        for match in matches:\n","            tag_type, text_info, normalized_date = match\n","            relative_start_index = content_text.find(text_info.strip())\n","            if relative_start_index == -1 and tag_type == \"TIME\":\n","                # 對於時間類型，特別處理以匹配格式\n","                time_match = re.search(r'\\d{1,2}:\\d{2}\\w*m on \\d{1,2}\\.\\d{1,2}\\.\\d{2}', content_text)\n","                if time_match:\n","                    text_info = time_match.group()\n","                    relative_start_index = content_text.find(text_info.strip())\n","\n","            if relative_start_index != -1:\n","                actual_start_index = start_pos + relative_start_index\n","                actual_end_index = actual_start_index + len(text_info.strip())\n","\n","                reformatted_line = f\"{file_id}\\t{tag_type}\\t{actual_start_index}\\t{actual_end_index}\\t{text_info.strip()}\"\n","                if normalized_date:\n","                    reformatted_line += f\"\\t{normalized_date}\"\n","                reformatted_line += \"\\n\"\n","                reformatted_content.append(reformatted_line)\n","\n","    return reformatted_content\n","\n","reformatted_content = clean_and_reformat_content(normalized_date_time)"]},{"cell_type":"code","execution_count":null,"id":"7435656f-fd97-45d1-9960-f20fd99b6230","metadata":{"tags":[],"id":"7435656f-fd97-45d1-9960-f20fd99b6230"},"outputs":[],"source":["# Writing the reformatted content to a text file\n","txt_file_path = f\"./Result/{model_name}-{config['generation_max_length']}-answer.txt\"\n","\n","with open(txt_file_path, 'w') as file:\n","    for line in reformatted_content:\n","        file.write(line)"]},{"cell_type":"code","execution_count":null,"id":"7d2ce692-249b-4e69-a17b-f4c3f88d716a","metadata":{"id":"7d2ce692-249b-4e69-a17b-f4c3f88d716a"},"outputs":[],"source":["zip_file_path = f\"./Result/{model_name}-{config['generation_max_length']}-answer.txt.zip\"\n","\n","# Save zip file\n","with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n","    zipf.write(txt_file_path, arcname='answer.txt')\n"]},{"cell_type":"code","execution_count":null,"id":"a5ed7d9b-211d-4da3-abd6-f302a9c9cda9","metadata":{"id":"a5ed7d9b-211d-4da3-abd6-f302a9c9cda9"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}